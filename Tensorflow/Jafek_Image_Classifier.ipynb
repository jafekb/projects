{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from matplotlib import pyplot as plt \n",
    "import cPickle\n",
    "from sklearn import model_selection\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cifar_data(cifar_data_location):\n",
    "    def unpickle( file ):\n",
    "        fo = open(file, 'rb')\n",
    "        dict = cPickle.load(fo)\n",
    "        fo.close()\n",
    "        return dict\n",
    "\n",
    "    data = unpickle( cifar_data_location )\n",
    "\n",
    "    features = data['data']\n",
    "    labels = data['labels']\n",
    "    labels = np.atleast_2d( labels ).T\n",
    "\n",
    "    return labels, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv( x, filter_size=2, stride=2, num_filters=128, is_output=False, name=\"conv\" ):\n",
    "    '''\n",
    "    x is an input tensor\n",
    "    Declare a name scope using the \"name\" parameter\n",
    "    Within that scope:\n",
    "      Create a W filter variable with the proper size\n",
    "      Create a B bias variable with the proper size\n",
    "      Convolve x with W by calling the tf.nn.conv2d function\n",
    "      Add the bias\n",
    "      If is_output is False,\n",
    "        Call the tf.nn.relu function\n",
    "      Return the final op\n",
    "    ''' \n",
    "    x_shape = x.get_shape().as_list()\n",
    "\n",
    "    with tf.name_scope(name) as scope:\n",
    "        W = tf.get_variable(name='W_{}'.format(name), shape=[filter_size, filter_size, x_shape[-1], num_filters], initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "        b = tf.get_variable(name='b_{}'.format(name), shape=[num_filters], initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "\n",
    "        convolution = tf.nn.conv2d(input=x, filter=W, strides=[1,stride, stride,1], padding='SAME', use_cudnn_on_gpu=None, data_format=None, name=None)\n",
    "        convolution = convolution + b\n",
    "\n",
    "        if is_output == 'False':\n",
    "            activated_output = tf.nn.relu(features=convolution, name=None)\n",
    "            return activated_output #This actually returns the whole computation graph that makes this.\n",
    "  \n",
    "        return convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc( x, out_size=150, is_output='False', name=\"fc\" ): #Bigger outsize makes it converge higher, but slower.\n",
    "    '''\n",
    "    x is an input tensor\n",
    "    Declare a name scope using the \"name\" parameter\n",
    "    Within that scope:\n",
    "    Create a W filter variable with the proper size\n",
    "    Create a B bias variable with the proper size\n",
    "    Multiply x by W and add b\n",
    "    If is_output is False,\n",
    "    Call the tf.nn.relu function\n",
    "    Return the final op\n",
    "    '''\n",
    "    x_shape = x.get_shape().as_list()\n",
    "\n",
    "    with tf.name_scope(name) as scope:\n",
    "        W = tf.get_variable(name='W_{}'.format(name), shape=[x_shape[-1], out_size], initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "        b = tf.get_variable(name='b_{}'.format(name), shape=[out_size], initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "\n",
    "        h = tf.matmul(x,W) + b\n",
    "\n",
    "        if is_output == 'False':\n",
    "            activated_output = tf.nn.relu(features=h, name=None)\n",
    "            return activated_output\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10 epochs is enough to get it over the minimum requirement of 20% accuracy, but \n",
    "#if you bump it up to 20 epochs, we can approach 50% accuracy.\n",
    "num_epochs = 30\n",
    "\n",
    "\n",
    "#Create DNN\n",
    "with tf.Graph().as_default():\n",
    "    #PLAYERS\n",
    "    x = tf.placeholder(tf.float32, shape=[1, 32, 32, 3], name='features') #Number parameters\n",
    "    y_ = tf.placeholder(tf.int64, shape=[1], name='labels') #Number labels.\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    #GAME\n",
    "    h_conv1 = conv(x, filter_size=4, num_filters=128, name='conv1')\n",
    "    h_pool1 = tf.nn.max_pool(h_conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "\n",
    "    h_conv2 = conv(h_pool1, filter_size=4, num_filters=64, name='conv2')\n",
    "    h_pool2 = tf.nn.max_pool(h_conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    h_pool2_drop = tf.nn.dropout(h_pool1, keep_prob)\n",
    "    \n",
    "    h_conv3 = conv(h_pool2_drop, filter_size=3, num_filters=64, name='conv3')\n",
    "    h_pool3 = tf.nn.max_pool(h_conv3, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    \n",
    "    h_conv4 = conv(h_pool3, filter_size=2, num_filters=32, name='conv4')\n",
    "    h_pool4 = tf.nn.max_pool(h_conv4, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    h_pool4_flat = tf.reshape(h_pool4, [-1, 32])\n",
    "\n",
    "    fc_1 = fc(h_pool4_flat, name='fc1')\n",
    "    fc_1_drop = tf.nn.dropout(fc_1, keep_prob) #Dropout tends to be used only after the first \n",
    "    # layer of its type. Is that just by practice, or by theory?\n",
    "    \n",
    "    fc_2 = fc(fc_1_drop, name='fc2')\n",
    "    \n",
    "    fc_3 = fc(fc_2, name='fc3')\n",
    "    \n",
    "    fc_4 = fc(fc_3, name='fc4')\n",
    "    \n",
    "    fc_5 = fc(fc_4, name='fc5')\n",
    "    \n",
    "    fc_6 = fc(fc_5, name='fc6')\n",
    "\n",
    "    logits = fc(fc_6, out_size=10, is_output='True', name='fc7')\n",
    "    y_guess = tf.argmax(logits, axis=1)\n",
    "\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_, logits=logits))\n",
    "\n",
    "    #TRAIN\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "    correct_prediction = tf.equal(y_guess, y_)\n",
    "    accuracy = tf.cast(correct_prediction, tf.float32)\n",
    "\n",
    "    #DATA\n",
    "    cifar_labels, cifar_features = get_cifar_data('cifar-10-batches-py/data_batch_3') #just 1 batch.\n",
    "    cifar_features_whitened = (cifar_features - np.mean(cifar_features)) / np.std(cifar_features)\n",
    "    cifar_features_whitened = cifar_features_whitened.reshape([len(cifar_features_whitened), 1, 32, 32, 3])\n",
    "    x_train, x_test, y_train, y_test = model_selection.train_test_split(cifar_features_whitened, cifar_labels, test_size=0.2)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        all_test_accuracies, all_train_accuracies = [], []\n",
    "\n",
    "        for _ in tqdm(xrange(num_epochs)):\n",
    "            this_iter_test_accuracy, this_iter_train_accuracy = [], []\n",
    "\n",
    "            for train_ind, train_image in enumerate(x_train):\n",
    "                train_accuracy, _ = sess.run([accuracy, train_step], feed_dict={x:train_image, y_:y_train[train_ind], keep_prob:0.5}) \n",
    "                this_iter_train_accuracy.append(train_accuracy)\n",
    "\n",
    "            for test_ind, test_image in enumerate(x_test):\n",
    "                test_accuracy = sess.run([accuracy], feed_dict={x:test_image, y_:y_test[test_ind], keep_prob:1.0}) #When we test it, we want to keep the whole network.\n",
    "                this_iter_test_accuracy.append(test_accuracy)\n",
    "\n",
    "            all_test_accuracies.append(np.mean(this_iter_test_accuracy))\n",
    "            all_train_accuracies.append(np.mean(this_iter_train_accuracy))\n",
    "\n",
    "        writer = tf.summary.FileWriter(\"image_classifier_graph\", sess.graph)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bests.append(np.max(all_test_accuracies))\n",
    "print all_bests\n",
    "\n",
    "plt.plot(all_test_accuracies, 'r', label='Test set')\n",
    "plt.plot(all_train_accuracies, 'b', label='Train set')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
