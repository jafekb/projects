{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tools\n",
    "import re\n",
    "import gensim\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import sys\n",
    "from tqdm import tqdm_notebook as tqdmn\n",
    "from collections import Counter\n",
    "import glob\n",
    "from scipy import spatial\n",
    "import time\n",
    "\n",
    "#Word processors.\n",
    "import wikipedia\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia\n",
    "My first thought was that a good way to get word associations would be to use Python's Wikipedia package, whose 'search' function returns all the links to other subjects on the page which you input (see search for 'labyrinth' below).\n",
    "\n",
    "When the first iteration didn't work, I thought that I would try using only links which consist of 1 word. This was better, but as you can see there are still a lot of false positives. I think we can find a better way. We'll leave this method as a possibility, but look at some other methods. I think NLTK will be a good package to give us word associations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Clay pigeon shooting', 'Clay pigeon', 'Clay Pigeons', 'The Clay Pigeon', 'Clay Pigeon (film)', 'Sporting clays', 'Beretta Silver Pigeon', 'Clay Pigeon Shooting Association', 'Clay pigeon floor procedure', 'Passenger pigeon']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = wikipedia.search('clay pigeon')\n",
    "print (l)\n",
    "newl = []\n",
    "for i in l:\n",
    "    if len(i.split(' ')) == 1:              #Only accept one-word responses.\n",
    "        if i.lower() != searchword.lower(): #Don't accept copies of the search word.\n",
    "            newl.append(i)\n",
    "newl\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordNet\n",
    "**Defn**: a _hypernym_ is a word with a broad meaning that more specific words fall under; a superordinate. For example, _color_ is a hypernym of _red_ (source: Google dictionary).\n",
    "\n",
    "I think this one is really good, especially since Python's NLTK package already has a large base of related words. This implementation gets synonyms and hypernyms. If you think of words as having a family tree of specificity (where the leaves are the most specific word), then synonymns are like cousins, nd hypernyms are like parents or grandparents. \n",
    "\n",
    "This code works very well for words that have only one well-defined meaning, like *mitten* or *coffee*, but not very good for words that either have many meanings or are abstract, such as *mint* or *blue*.\n",
    "\n",
    "The worst-case scenario for this code is when a word is not in WordNet's corpus, in which case it can't do anything. One big problem is that WordNet's corpus only includes single words. That means we can't use it for phrases like *electrical device* or *clay pigeon*. In these cases, we will have to do something else (maybe use Wikipedia for this?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WordnetAssociations(object):\n",
    "    \"\"\"\n",
    "    This class is designed to tell us words which are similar to a given word.\n",
    "    Its main function takes a random word as input, and returns up to 5 words \n",
    "    which are similar.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, searchword):\n",
    "        self.sw = searchword\n",
    "        self.sw_all_syns = wn.synsets(searchword)\n",
    "        \n",
    "        if len(self.sw_all_syns) == 0:\n",
    "            raise ValueError('That word is not in WordNet\\'s corpus')\n",
    "        \n",
    "        self.sw_best_syn = self.sw_all_syns[0]    #I'm going to assume the best one is first.\n",
    "        \n",
    "    def get_synsets(self):\n",
    "        return wn.synsets(self.sw)\n",
    "    \n",
    "    def associated_words(self, SYNS):\n",
    "        \"\"\"\n",
    "        This function returns all synonyms and their hypernyms of the search word\n",
    "        which was input to the class.\n",
    "        \"\"\"\n",
    "        ASSOC_WORDS = self.sw_all_syns\n",
    "        for syn in ASSOC_WORDS:\n",
    "            ASSOC_WORDS.extend(syn.hypernyms())\n",
    "            \n",
    "        return list(set(ASSOC_WORDS))\n",
    "    \n",
    "    def metric(self, synset2):\n",
    "        \"\"\"\n",
    "        This function tells us how similar (in Wu-Palmer similarity) a given word\n",
    "        is to the original search word.\n",
    "        Sometimes, wup_similarity returns None. I don't understand completely why \n",
    "        this happens, but basically if the too words are too dissimilar, then they\n",
    "        won't have any connection between them. In this case, instead of returning \n",
    "        'None', this function returns 0.\n",
    "        \n",
    "        It looks like Wu-Palmer similarity will work the best, since\n",
    "        1) path_similarity doesn't differentiate enough between different words,\n",
    "             e.g., there are a lot of values of 0.66666, 0.5, 0.333333.\n",
    "        2) lch_similarity doesn't work with words that don't have the same part of speech\n",
    "        \"\"\"\n",
    "        val = wn.wup_similarity(self.sw_best_syn, synset2)\n",
    "        if not val:\n",
    "            return 0\n",
    "        return val\n",
    "    \n",
    "    def sort_by_similarity(self, words):\n",
    "        \"\"\"\n",
    "        Input: All of the words which have been indicated to be similar to \n",
    "        the search word.\n",
    "        \n",
    "        Output:\n",
    "        Sorts the list, according to HOW similar each of the words are to\n",
    "        the search word. Then, it returns only the top 5 words.\n",
    "        \n",
    "        Note: One or more of those words might be the word itself, so it will\n",
    "        be taken out in the syns_to_strs function, but that's ok because 4 related words\n",
    "        is better than having too many, in the case where there are no repeated words.\n",
    "        \"\"\"\n",
    "        \n",
    "        return sorted(words, key=self.metric)[::-1][:5] \n",
    "    \n",
    "    def syns_to_strs(self, SYN_final, thresh=0.4):\n",
    "        final = []\n",
    "        for syn in SYN_final:\n",
    "            word = syn.name().split('.')[0] #Get rid of the POS tag.\n",
    "            word = word.split('_')[0] #TODO this might be a bad idea. We're adding the adjective.\n",
    "            if word != self.sw:\n",
    "                final.append(word)\n",
    "        return final\n",
    "    \n",
    "    def main(self):\n",
    "        SYNS = self.get_synsets()\n",
    "        all_words_extracted = self.associated_words(SYNS)\n",
    "        final_as_synsets = self.sort_by_similarity(all_words_extracted)\n",
    "        return self.syns_to_strs(final_as_synsets, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNet should work well for our task. Now let's implement it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I copied the data into a tab-separated .txt file because that allowed Pandas to read it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_copy.txt', delimiter='\\t')\n",
    "vals = df.values\n",
    "m,n = vals.shape\n",
    "output = np.empty([m, 3]).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33393b56c46f40acbb02bda455454b2c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for ind in tqdmn(range(len(vals))): #TQDM can only handle simple loops. \n",
    "    row = vals[ind]\n",
    "    classification, tags = row\n",
    "    class_id = classification.split(' ')[0]\n",
    "    orig_word = ' '.join(classification.split(' ')[1:])\n",
    "    searchwords = re.split(r' |, ', classification)[1:]\n",
    "    first = searchwords[0]\n",
    "    \n",
    "    #Now, come to think of it, it'd be better not to initialize a new class every time...\n",
    "    try:\n",
    "        W = WordnetAssociations(first) \n",
    "        associated_words = W.main()\n",
    "    except ValueError: #\"That word is not in the corpus\"\n",
    "        \"\"\"\n",
    "        We're going to use wikipedia.\n",
    "        This WILL be pretty bad, there must be a better way.\n",
    "        \"\"\"\n",
    "        l = wikipedia.search(first)\n",
    "        associated_words = []\n",
    "        for i in l:\n",
    "            if len(i.split(' ')) == 1: #Only accept one-word responses.\n",
    "                if i.lower() != first.lower(): #Don't accept copies of the search word.\n",
    "                    associated_words.append(i)\n",
    "    output[ ind ] = [class_id, orig_word, ', '.join(associated_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_df = pd.DataFrame(output)\n",
    "out_df.to_csv('word_associations2.csv', header=['ID', 'Word', 'Tags'], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "This would probably be great, but my laptop overflows with memory trying to read in the embeddings..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe\n",
    "https://nlp.stanford.edu/pubs/glove.pdf  \n",
    "This is similar to word2vec. They have a lot of embeddings, the one I'm using is from Wikipedia and it's their smallest one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4499c9228102405cbe74a4c13e26690d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17dcea910eb249a18279df27949ed920"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dc78d5da26f46c396455b1bd950b257"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d3671d4d81843d98749df3d7c54c7ad"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "724b1c553839446495b33da7cbad0190"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#DO NOT RUN THIS CELL AGAIN\n",
    "datadict = {}\n",
    "FILES = glob.glob('glove*.txt')\n",
    "\n",
    "for file in tqdmn(FILES):\n",
    "    with open(file) as f:\n",
    "        for ind, l in tqdmn(enumerate(f)):\n",
    "            line = l.split(' ')\n",
    "            word, vec = line[0], np.array(line[1:]).astype(np.float32)\n",
    "            datadict[word] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary maintains order: True\n"
     ]
    }
   ],
   "source": [
    "#Now we'll put all the vectors in a KDTree\n",
    "glove_lk = list(datadict.keys())\n",
    "glove_vals = list(datadict.values())\n",
    "mykey = lk[1000]\n",
    "myvec = glove_vals[1000]\n",
    "print ('Dictionary maintains order:', np.all(myvec == datadict[mykey]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NumPy arrays will be an easier datatype generally.\n",
    "WORDS = np.array(lk)\n",
    "A = np.array(glove_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.307508945465088\n"
     ]
    }
   ],
   "source": [
    "#Make the KDTree for vector comparison\n",
    "start = time.time()\n",
    "tree = spatial.KDTree(A)\n",
    "elapsed = time.time() - start\n",
    "print (elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce4bbb6405b4465197777746d33ba1a2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"//anaconda/envs/py36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"//anaconda/envs/py36/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"//anaconda/envs/py36/lib/python3.6/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-11f92def28d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m#Glove takes a little longer to run, but the words look better.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mmyvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatadict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyvec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mdists\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0massociated_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWORDS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/scipy/spatial/kdtree.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, x, k, eps, p, distance_upper_bound)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mii\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mhits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistance_upper_bound\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdistance_upper_bound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhits\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhits\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/scipy/spatial/kdtree.py\u001b[0m in \u001b[0;36m__query\u001b[0;34m(self, x, k, eps, p, distance_upper_bound)\u001b[0m\n\u001b[1;32m    353\u001b[0m                 \u001b[0;31m# brute-force\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m                 \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminkowski_distance_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mdistance_upper_bound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/scipy/spatial/kdtree.py\u001b[0m in \u001b[0;36mminkowski_distance_p\u001b[0;34m(x, y, p)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Now iterate through all the words, make a CSV\n",
    "dists = []\n",
    "start = time.time()\n",
    "\n",
    "for ind in tqdmn(range(len(vals))): #TQDM can only handle simple loops. \n",
    "    row = vals[ind]\n",
    "    classification, tags = row\n",
    "    class_id = classification.split(' ')[0]\n",
    "    orig_word = ' '.join(classification.split(' ')[1:])\n",
    "    searchwords = re.split(r' |, ', classification)[1:]\n",
    "    first = searchwords[0]\n",
    "    \n",
    "    #Now, come to think of it, it'd be better not to initialize a new class every time...\n",
    "    try:\n",
    "        #Glove takes a little longer to run, but the words look better.\n",
    "        myvec = datadict[first]\n",
    "        ds, inds = tree.query(myvec, 6)\n",
    "        dists.append(ds)\n",
    "        associated_words = list(WORDS[inds][1:])\n",
    "    except KeyError: #\"That word is not in the corpus\"\n",
    "        #If it isn't in our list, we're just going to ignore it for now.\n",
    "        associated_words = []\n",
    "        \n",
    "    output[ ind ] = [class_id, orig_word, ', '.join(associated_words)]\n",
    "    \n",
    "out_df = pd.DataFrame(output)\n",
    "out_df.to_csv('glove_association.csv', header=['ID', 'Word', 'Tags'], index=False)\n",
    "\n",
    "print (time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
