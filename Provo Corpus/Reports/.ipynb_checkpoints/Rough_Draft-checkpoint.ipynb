{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eye Fixation Prediction in Digital Images\n",
    "Benjamin Jafek <br>\n",
    "Dec 2, 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I seek to study eye-tracking data to answer the question, “How do people filter out unnecessary information in digital images?” I have eye-tracking data from a large group of volunteers who were given two separate tasks, a scene viewing task and a visual search task. I anticipate that these findings will not only provide insight into cognitive vision, but also suggest sampling techniques for computer vision algorithms. I will test my findings on state-of-the-art computer vision algorithms, comparing accuracy and speed between baseline and sampled images.\n",
    "<img style=\"float: right;width:500px;height:300px;\" align=\"right\" src=\"licenseplate_eyetrack.png\">\n",
    "<br> <br>\n",
    "Despite the recent leaps and bounds in recent computer vision abilities, computers are still far outpaced by the human visual system. With vision being our primary sensory input, we humans each develop the ability to effortlessly create full visual fields from relatively small amounts of information. Is there not something which computers can learn from this exemplary system? The field of computational cognition has recently begun exploring this fascinating question. In fact, this past October, a conference entitled ‘Mutual Benefits of Cognitive and Computer Vision’  took place in Venice, Italy, with researchers from all over the world presenting on topics such as “What are the visual features underlying human versus machine vision?” [1], “Evaluation of deep learning on an abstract image classification dataset” [2], and “Can we speed up 3D scanning? A cognitive and geometric analysis” [3]. In my project, I hope to continue this exciting trend. The long-term purpose of this project is to explore how human data can be used to improve the efficiency of DCNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This dataset required a lot of feature engineering.** <br><br>\n",
    "The features of the original dataset were things like 'current x fixation', 'current y fixation', 'saccade length', and 'saccade angle': features describing the coordinates of the eye fixation. This information is helpful, but certainly nothing that you could feed into a learning algorithm and hope to get meaningful results from. <br>\n",
    "First, we must ask 'Are eye fixations a good measure of visual attention? Is it both necessary and sufficient to fixate the eye on a location in order to attend to it?' This is a very interesting question. The answer is that we're not completely sure, but it has been found to be the best proxy for attention [1]. Alternatively, we could maybe hook up an electroencephalogram to the patient, but even then, modern science is not entirely sure what the brain mechanisms are that correspond to visual attention [2]. Thus, we will assume that eye fixations are necessary and sufficient conditions for visual attention for the remainder of this project.\n",
    "For my features, I looked at low-level features of the image in a neighborhood near the eye fixation. I let this neighborhood be a ball of radius 75 pixels, which corresponds pretty well to how much peripheral vision you have when you are focusing on a single point. <br>\n",
    "Thus, I had a lot of images that looked something like this, where the red ball is centered at the human eye fixation:\n",
    "<center><img style=\"float: center;width:800px;height:300px;\" align=\"right\" src=\"cutout.png\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For this specific project, I plan to develop a model that can predict eye fixations in an image.\n",
    "Now, deciding where to look in a scene requires a very high-level understanding of the scene. For example, in the above image, bicycles and cars are more interesting than piles of snow and roads. However, there is no 'bicycle' feature that I could extract from this image. Well, there is, but it requires many layers of a deep neural network to extract [6]. The whole point of this project is to develop some model of human visual attention (which generally relies on high-level understanding) using low-level features of the image.  \n",
    "Thus, my hypothesis is that some combination of low-level data will correlate with visual attention strongly enough that we can predict eye fixations on a novel image.  \n",
    "The features that I chose to extract from the localized image are:  \n",
    "\n",
    "1. Picture name\n",
    "2. X center (of localization)\n",
    "3. Y center (of localization\n",
    "4. Brightness (calculated by the mean value of the V channel in a HSV image).\n",
    "5. Contrast (RMS).\n",
    "6. Focus measure - basically 'how blurry the image is' - (calculated by the variance of the Laplacian transform [7]). \n",
    "7. RMS contrast on the vertical edges (Sobel X edge detector)\n",
    "8. Focus measure on the vertical edges (Sobel X edge detector)\n",
    "9. RMS contrast on the horizontal edges (Sobel Y edge detector)\n",
    "10. Focus measure on the horizontal edges (Sobel Y edge detector) <br>\n",
    "11. Red pixels (% of total)\n",
    "12. Green pixels (% of total)\n",
    "13. Blue pixels (% of total)\n",
    "14. Indoor/outdoor\n",
    "15. Fixation at this location? (Y/N) **label**\n",
    "\n",
    "I iterated through every 5 pixels (i.e., 0,5,10,...,800) of each of the 32 images, and gathered these 11 features at each point. The 'label' of the data will be EYE FIXATION: Y/N. \n",
    "<img style=\"float: left;width:300px;height:200px;PADDING-RIGHT:10px;PADDING_TOP:10px;\" align=\"right\" src=\"attic.jpg\">\n",
    "I anticipate that using this information, we will be able to accurately predict the eye fixations of a person given a novel image. <br>\n",
    "\n",
    "If we can manage that, then we'll have a good way of modeling high-level visual decisions using low-level image features. That would be exciting. I will continue to search for other low-level properties of images to feed into my model, but for now, these are the ones that I have. Notice that the 14th feature, indoor/outdoor, is a high-level property of the image, and will be hand-labeled. This is, of course, cheating, but it is a very easy label, and should be knowable information in real-world applications. In the table below, the image is of an attic, so it is indoors, so the 'Outdoor' boolean variable is False, corresponding to a 0.\n",
    "\n",
    "Below is listed the CSV corresponding to the image to the left:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Picture</th>\n",
       "      <th>X_center</th>\n",
       "      <th>Y_center</th>\n",
       "      <th>Brightness</th>\n",
       "      <th>Contrast</th>\n",
       "      <th>FM</th>\n",
       "      <th>VerticalContrast</th>\n",
       "      <th>VerticalFM</th>\n",
       "      <th>HorizontalContrast</th>\n",
       "      <th>HorizontalFM</th>\n",
       "      <th>Outdoor</th>\n",
       "      <th>Blue</th>\n",
       "      <th>Green</th>\n",
       "      <th>Red</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>attic.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.002277</td>\n",
       "      <td>0.007155</td>\n",
       "      <td>0.031867</td>\n",
       "      <td>3.400914</td>\n",
       "      <td>0.030676</td>\n",
       "      <td>4.938998</td>\n",
       "      <td>0</td>\n",
       "      <td>0.324017</td>\n",
       "      <td>0.330211</td>\n",
       "      <td>0.345772</td>\n",
       "      <td>1294570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>attic.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.002203</td>\n",
       "      <td>0.006928</td>\n",
       "      <td>0.030702</td>\n",
       "      <td>3.615816</td>\n",
       "      <td>0.028437</td>\n",
       "      <td>4.945617</td>\n",
       "      <td>0</td>\n",
       "      <td>0.323588</td>\n",
       "      <td>0.330003</td>\n",
       "      <td>0.346409</td>\n",
       "      <td>1414482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>attic.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.002146</td>\n",
       "      <td>0.007204</td>\n",
       "      <td>0.030357</td>\n",
       "      <td>3.715107</td>\n",
       "      <td>0.027904</td>\n",
       "      <td>4.993102</td>\n",
       "      <td>0</td>\n",
       "      <td>0.323025</td>\n",
       "      <td>0.330075</td>\n",
       "      <td>0.346900</td>\n",
       "      <td>1540524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>attic.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.002090</td>\n",
       "      <td>0.006742</td>\n",
       "      <td>0.029232</td>\n",
       "      <td>3.655078</td>\n",
       "      <td>0.026608</td>\n",
       "      <td>4.859082</td>\n",
       "      <td>0</td>\n",
       "      <td>0.322190</td>\n",
       "      <td>0.329585</td>\n",
       "      <td>0.348225</td>\n",
       "      <td>1664647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>attic.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.002031</td>\n",
       "      <td>0.006820</td>\n",
       "      <td>0.028192</td>\n",
       "      <td>3.668993</td>\n",
       "      <td>0.024983</td>\n",
       "      <td>4.775179</td>\n",
       "      <td>0</td>\n",
       "      <td>0.321549</td>\n",
       "      <td>0.328777</td>\n",
       "      <td>0.349674</td>\n",
       "      <td>1779742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Picture  X_center  Y_center  Brightness  Contrast        FM  \\\n",
       "0  attic.jpg         0         0    0.000206  0.002277  0.007155   \n",
       "1  attic.jpg         0         5    0.000208  0.002203  0.006928   \n",
       "2  attic.jpg         0        10    0.000211  0.002146  0.007204   \n",
       "3  attic.jpg         0        15    0.000212  0.002090  0.006742   \n",
       "4  attic.jpg         0        20    0.000213  0.002031  0.006820   \n",
       "\n",
       "   VerticalContrast  VerticalFM  HorizontalContrast  HorizontalFM  Outdoor  \\\n",
       "0          0.031867    3.400914            0.030676      4.938998        0   \n",
       "1          0.030702    3.615816            0.028437      4.945617        0   \n",
       "2          0.030357    3.715107            0.027904      4.993102        0   \n",
       "3          0.029232    3.655078            0.026608      4.859082        0   \n",
       "4          0.028192    3.668993            0.024983      4.775179        0   \n",
       "\n",
       "       Blue     Green       Red    Total  \n",
       "0  0.324017  0.330211  0.345772  1294570  \n",
       "1  0.323588  0.330003  0.346409  1414482  \n",
       "2  0.323025  0.330075  0.346900  1540524  \n",
       "3  0.322190  0.329585  0.348225  1664647  \n",
       "4  0.321549  0.328777  0.349674  1779742  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../FULLCSVs/attic.csv', index_col=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "<center><img style=\"float: center;width:400px;height:300px;\" align=\"right\" src=\"image_gradients.png\"></center>\n",
    "As mentioned above, two image processing techniques I used to analyze the images were\n",
    "(1) the Laplacian operator, and (2) the Sobel Edge detector. Both of these techniques basically simplify the image down to just the edges, so that we can analyze the edges better. The Laplacian operator is used to calculate focus measure. Notice that the Sobel X edge detector leaves only vertical lines, and the Sobel Y detector leaves only horizontal lines. This allows us to analyze vertical and horizontal lines separately.\n",
    "<br> <br> <br><br> <br> <br><br> <br> <br>\n",
    "\n",
    "<center><img style=\"float: center;width:600px;height:400px;\" align=\"right\" src=\"rgb_ims.png\"></center>\n",
    "Next, I split the image into its Red, Green, and Blue channels. To the right we see the attic.jpg image. Since the image is comprised of all colors of the spectrum and not just RGB, it is difficult to tell exactly which colors contribute to which shade of brown in the attic. In fact, most of the percentages of RGB remain right around 33% for each. However, there are some outliers. The below images show the most Red, Green, and Blue segments of the image, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rimg, grimb, blimg = plt.imread('red_img.png'), plt.imread('green_img.png'), plt.imread('blue_img.png')\n",
    "plt.subplot(131)\n",
    "plt.ims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dec 1: Data visualization due\n",
    "\n",
    "Plot the data in various relevant ways.\n",
    "Describe clearly what these plots tell you.\n",
    "Use the plots to identify outliers, bad data, etc\n",
    "Use the plots to identify patterns and relations in the data.\n",
    "Repeat the process as necessary to get a clear view of what is going on.\n",
    "At this point, you should have a rough draft of your final report. \n",
    "    The final product should loosely follow this outline:\n",
    "\n",
    "Introduction: briefly describe the data, why it might be interesting, and what you hope to find out about it.\n",
    "Data Collection: describe the source of the data and the methods used for gathering and cleaning it, \n",
    "    including feature engineering.\n",
    "Data Visualization: plots and analysis of the data.\n",
    "Conclusion: what are the main takeaways from your analysis?\n",
    "Remember that the final draft may be no longer than 12 pages. \n",
    "    Choose carefully which visualizations to use, what information to print, and so on.\n",
    "\n",
    "Finally, your rough draft needs to be delivered to your peer review group \n",
    "    (which we will assign) so that you can give feedback and have \n",
    "    discussion in lab on Thursday, December 8th."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "1.\tD. Linsley, S. Eberhardt, T. Sharma, P. Gupta, T. Serre. “What are the visual features underlying human versus machine vision?” _Mutual Benefits of Cognitive and Computer Vision_. 2017.\n",
    "2.\tS. Stabinger, A. Rodriguez. “Evaluation of Deep Learning on an Abstract Image Classification Dataset.” _Mutual Benefits of Cognitive and Computer Vision_. 2017.\n",
    "3.\tK. Vaiapury, B. Purushothaman, A. Pal, S. Agarwal. “Can we speed up 3D scanning? A cognitive and geometric analysis.” _Mutual Benefits of Cognitive and Computer Vision_. 2017.\n",
    "4. Hoffman, J. and Subramaniam, B. \"The role of visual attention in saccadic eye movements.\" _Perception & Psychophysics_, 1995. <br>\n",
    "5. Busch, N. and VanRullen, R. \"Spontaneous EEG oscillations reveal periodic sampling of visual attention.\" _Psychological and Cognitive Sciences_, 2010. <br>\n",
    "6. Zhou, B., Khosla, A., Lapedriza, A., Oliva,  A., and Torralba, A. \"Learning Deep Features for Discriminative Localization.\" _CVPR_, 2016. <br>\n",
    "7. Pertuz, S., Puig, D., and Garcia, M. \"Analysis of focus measure operators for shape-from-focus.\" _Pattern Recognition_, 2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
