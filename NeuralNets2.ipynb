{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Nets 2\n",
    "Ben Jafek  \n",
    "MATH 404  \n",
    "2/26/18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "from keras.datasets import fashion_mnist as fmn\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "Experiment with fully connected neural nets for classification of the Fashion-MNIST data: add at least two more layers, make all hidden layers at least 20 neurons wide, and try it with both ReLU and sigmoid activations.  Train for as many epochs as you need until the loss function (categorical cross entropy) stops improving--Keras's `callbacks.EarlyStopping` may be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fmn.load_data()\n",
    "#Flatten the inputs to L*784 instead of L*28*28\n",
    "x_train = x_train.reshape(len(x_train), 28*28)\n",
    "x_test = x_test.reshape(len(x_test), 28*28)\n",
    "\n",
    "# Convert labels to categorical one-hot encoding\n",
    "one_hot_train_labels = to_categorical(y_train, num_classes=10)\n",
    "one_hot_test_labels = to_categorical(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relu\n",
      "sigmoid\n"
     ]
    }
   ],
   "source": [
    "ACCS = []\n",
    "TIMES = []\n",
    "num_epochs = 30\n",
    "ovr_start = time.time()\n",
    "\n",
    "for act in ['relu', 'sigmoid']:\n",
    "    print (act)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=784))\n",
    "    model.add(Activation(act))\n",
    "\n",
    "    model.add(Dense(32, input_dim=64))\n",
    "    model.add(Activation(act))\n",
    "\n",
    "    model.add(Dense(32, input_dim=32))\n",
    "    model.add(Activation(act))\n",
    "\n",
    "    model.add(Dense(16, input_dim=32))\n",
    "    model.add(Activation(act))\n",
    "\n",
    "    model.add(Dense(10, input_dim=16))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model, iterating on the data in batches of 32 samples\n",
    "    start = time.time()\n",
    "    hist = model.fit(x_train, one_hot_train_labels, epochs=3, batch_size=32, verbose=0)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    #Now save it for future indulgence.\n",
    "    acc_arr = np.array(hist.history['acc']) \n",
    "    loss_arr = np.array(hist.history['loss'])\n",
    "    time_arr = np.array(elapsed)\n",
    "    np.save('NN2/{}_accs.npy'.format(act), acc_arr)\n",
    "    np.save('NN2/{}_time.npy'.format(act), time_arr)\n",
    "    np.save('NN2/{}_loss.npy'.format(act), loss_arr)\n",
    "    \n",
    "print ('TOTAL TIME ELAPSED: {:.3f}'.format(time.time() - ovr_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "Using the notation from class today (and from the video) calculate one iteration of backpropagation by hand (or code something to do it for you).  That is, calculate the forward pass and then the backward pass to compute both the output of the network for the current weights, and the gradient (with respect to the Ws and the bs) on a (fully connected) neural network with two hidden layers of 2 neurons each (ReLu activation), two inputs, and a single output layer having a sigmoid activation function. Use the input data $x=(1,-1)$, $y=1$, and assume the current weights are\n",
    "\n",
    "W^1 = 0.25  0.1\n",
    "            -0.2   0.9\n",
    "\n",
    "b^(1) =   0.1\n",
    "             -0.2\n",
    "\n",
    "W^(2) =  0.5   0.8\n",
    "               0.3   0.7\n",
    "\n",
    "b^(2) =  -0.3\n",
    "               0.1\n",
    "\n",
    "W^(3) =  0.1   -0.2\n",
    "\n",
    "b^(3) = 0.3$\n",
    "\n",
    "So the structure of the network looks something like this:\n",
    "\n",
    "            L_1      L_2      L_3\n",
    "            \n",
    "    x_0 ----> O ----> O ----> O ----> \n",
    "\n",
    "        \\  /        \\  /           /\n",
    "       \n",
    "         / \\         / \\         /\n",
    "        \n",
    "          x_1 ----> O ----> O\n",
    "\n",
    "where the Os here represent neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
